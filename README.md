# CASML RAG System - Generative AI Hackathon

Há»‡ thá»‘ng **Retrieval-Augmented Generation (RAG)** cho [CASML Generative AI Hackathon](https://www.kaggle.com/competitions/casml-generative-ai-hackathon).

## ğŸ“ Project Structure (Optimized)

```
CASML-Generative-AI-Hackathon/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Original PDF files
â”‚   â”‚   â””â”€â”€ book.pdf
â”‚   â”œâ”€â”€ processed/              # Cached chunks & embeddings
â”‚   â”‚   â”œâ”€â”€ chunks.pkl
â”‚   â”‚   â””â”€â”€ embeddings.npy
â”‚   â””â”€â”€ test_questions.json     # Test queries
â”‚
â”œâ”€â”€ models/                     # Saved models & indexes
â”‚   â”œâ”€â”€ faiss_index.bin         # FAISS vector index
â”‚   â””â”€â”€ chunk_texts.pkl         # Text chunks for retrieval
â”‚
â”œâ”€â”€ outputs/                    # Generated outputs
â”‚   â””â”€â”€ submission.csv          # Kaggle submission
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ rag_pipeline_modular.ipynb  # Main pipeline (build from scratch)
â”‚   â””â”€â”€ demo_qa.ipynb              # Quick demo (use pre-built index)
â”‚
â”œâ”€â”€ src/                        # Source code (modular components)
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ indexing/
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ generation/
â”‚   â””â”€â”€ evaluation/
â”‚
â”œâ”€â”€ config.yaml                 # Pipeline configuration
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Run Pipeline

**Option A: Build from scratch (5-10 min)**
```bash
jupyter notebook notebooks/rag_pipeline_modular.ipynb
# Run cells 1-8: Load â†’ Chunk â†’ Embed â†’ Index â†’ Retrieve
```

**Option B: Use pre-built index (instant)**
```bash
jupyter notebook notebooks/demo_qa.ipynb
# Load saved index and query immediately
```

## ğŸ“Š Pipeline Overview

### Current Implementation (Notebook)

1. **PDF Loading** - LangChain PyPDFLoader
2. **Chunking** - Recursive splitting (1000 chars, 200 overlap)
3. **Embedding** - BAAI/bge-large-en-v1.5 (1024 dims)
4. **Indexing** - FAISS IndexFlatIP (cosine similarity)
5. **Retrieval** - Two-stage:
   - FAISS: Fast search (50 candidates)
   - FlagReranker: Accurate reranking (top 5)

### Next Steps (To Complete)

6. **LLM Generation** - Add answer generation
7. **TOC Extraction** - Extract references from PDF
8. **Batch Processing** - Process all test queries
9. **Submission** - Generate CSV for Kaggle

## ğŸ”§ Key Features

- âœ… **Two-stage retrieval**: FAISS (speed) + FlagReranker (accuracy)
- âœ… **BGE embeddings**: State-of-the-art semantic search
- âœ… **No TensorFlow conflicts**: Pure PyTorch stack
- âœ… **GPU optimized**: sentence-transformers CUDA support
- ğŸ”¨ **Coming**: Index caching, LLM integration, TOC references

## ğŸ“ Usage Example

### Quick Retrieval Test
```python
# Already in notebook cells 6-8

# Search
query = "What did Freud contribute to psychology?"
query_emb = embedding_model.encode([query])
distances, indices = index.search(query_emb, k=50)

# Rerank
pairs = [[query, chunk_texts[idx]] for idx in indices[0]]
scores = reranker_model.compute_score(pairs)

# Top 5 results
for idx, score in top_5:
    print(f"Score: {score:.4f}")
    print(chunk_texts[idx][:200])
```

## ğŸ¯ Performance

- **Embedding**: ~100 chunks/sec (GPU)
- **FAISS search**: <2ms (2543 vectors)
- **Reranking**: ~100ms (50 candidates)
- **Total**: ~5 min for 645 pages

## ğŸ“š Tech Stack

- [sentence-transformers](https://www.sbert.net/) - Embeddings
- [FAISS](https://github.com/facebookresearch/faiss) - Vector search
- [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding) - Reranking
- [LangChain](https://python.langchain.com/) - PDF processing

## ğŸ”— Resources
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/                 # Truy xuáº¥t documents
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ retriever.py           # Dense/Sparse/Hybrid retrieval + Re-ranking
â”‚   â”‚
â”‚   â”œâ”€â”€ generation/                # LLM inference
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ generator.py           # LLMGenerator, RAGPipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/                # Metrics & submission
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ evaluator.py           # Evaluator, SubmissionGenerator
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                     # Helper functions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ helpers.py             # Logging, seeding, timing
â”‚   â”‚
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scripts/                       # Executable scripts
â”‚   â”œâ”€â”€ download_data.py           # Download Kaggle data
â”‚   â”œâ”€â”€ build_index.py             # Build embeddings & indexes
â”‚   â”œâ”€â”€ evaluate.py                # Evaluate on training set
â”‚   â”œâ”€â”€ generate_predictions.py    # Generate test predictions
â”‚   â””â”€â”€ run_pipeline.py            # Run end-to-end pipeline
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter notebooks
â”‚   â””â”€â”€ (EDA, experiments, visualization)
â”‚
â”œâ”€â”€ tests/                         # Unit tests
â”‚   â””â”€â”€ (pytest tests)
â”‚
â”œâ”€â”€ config.yaml                    # Cáº¥u hÃ¬nh chÃ­nh
â”œâ”€â”€ .env.example                   # Template cho biáº¿n mÃ´i trÆ°á»ng
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md                      # File nÃ y
```

---

## ğŸš€ CÃ i Ä‘áº·t

### 1. Clone repository
```bash
git clone https://github.com/your-username/CASML-Generative-AI-Hackathon.git
cd CASML-Generative-AI-Hackathon
```

### 2. Táº¡o virtual environment
```powershell
# Windows PowerShell
python -m venv venv
.\venv\Scripts\Activate.ps1

# Linux/Mac
python -m venv venv
source venv/bin/activate
```

### 3. CÃ i Ä‘áº·t dependencies
```bash
pip install -r requirements.txt
```

**LÆ°u Ã½**: 
- Náº¿u cÃ³ GPU CUDA, dÃ¹ng `tensorflow-gpu` vÃ  `faiss-gpu`:
  ```bash
  pip install tensorflow-gpu==2.15.0
  pip install faiss-gpu
  ```
- Kiá»ƒm tra CUDA vÃ  cuDNN tÆ°Æ¡ng thÃ­ch: https://www.tensorflow.org/install/source#gpu

### 4. Cáº¥u hÃ¬nh API keys (tÃ¹y chá»n)
```bash
cp .env.example .env
# Chá»‰nh sá»­a .env vÃ  thÃªm API keys (náº¿u dÃ¹ng OpenAI/Anthropic/Kaggle)
```

---

## ğŸ’» Sá»­ dá»¥ng

### Pipeline hoÃ n chá»‰nh (Recommended)

Cháº¡y toÃ n bá»™ pipeline tá»« data Ä‘áº¿n submission:

```bash
python scripts/run_pipeline.py
```

### Tá»«ng bÆ°á»›c riÃªng láº»

#### BÆ°á»›c 1: Download dá»¯ liá»‡u tá»« Kaggle
```bash
python scripts/download_data.py
```
*YÃªu cáº§u: KAGGLE_USERNAME vÃ  KAGGLE_KEY trong .env*

#### BÆ°á»›c 2: Build search index
```bash
python scripts/build_index.py
```
Táº¡o chunks, embeddings, vÃ  FAISS/BM25 indexes.

#### BÆ°á»›c 3: Evaluate trÃªn training set
```bash
python scripts/evaluate.py
```
ÄÃ¡nh giÃ¡ model vá»›i BLEU, ROUGE, BERTScore trÃªn táº­p train.

#### BÆ°á»›c 4: Generate predictions cho test set
```bash
python scripts/generate_predictions.py
```
Táº¡o file submission CSV trong `data/submissions/`.

---

## âš™ï¸ Cáº¥u hÃ¬nh

Táº¥t cáº£ cáº¥u hÃ¬nh Ä‘Æ°á»£c quáº£n lÃ½ trong **`config.yaml`**.

### CÃ¡c pháº§n chÃ­nh:

#### 1. **Indexing**
```yaml
indexing:
  chunking:
    strategy: "semantic"  # fixed, semantic, sentence, paragraph
    chunk_size: 512
    chunk_overlap: 50
  
  embedding:
    model_name: "sentence-transformers/all-MiniLM-L6-v2"
    # Hoáº·c dÃ¹ng fine-tuned model:
    # model_name: "your-username/your-finetuned-model"
    device: "cuda"
    batch_size: 32
  
  index:
    type: "hybrid"  # faiss, bm25, hybrid
```

#### 2. **Retrieval**
```yaml
retrieval:
  strategy: "hybrid"  # dense, sparse, hybrid
  top_k: 5
  dense_weight: 0.6
  sparse_weight: 0.4
  
  use_reranker: true
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  rerank_top_k: 3
```

#### 3. **Generation**
```yaml
generation:
  model:
    provider: "huggingface"
    model_name: "google/flan-t5-base"
    # Hoáº·c: "meta-llama/Llama-2-7b-chat-hf"
    # Hoáº·c: "your-username/your-finetuned-llm"
    device: "cuda"
    load_in_8bit: false  # Quantization Ä‘á»ƒ tiáº¿t kiá»‡m VRAM
  
  inference:
    max_new_tokens: 256
    temperature: 0.3
    top_p: 0.9
  
  prompt:
    template: |
      Answer the question based on the context below.
      
      Context: {context}
      Question: {question}
      Answer:
```

#### 4. **Evaluation**
```yaml
evaluation:
  metrics:
    - "bleu"
    - "rouge"
    - "bertscore"
```

---

## ğŸ§© MÃ´-Ä‘un chi tiáº¿t

### 1. **Config Module** (`src/config/`)
- **Vai trÃ²**: Load cáº¥u hÃ¬nh tá»« `config.yaml` vÃ  `.env`
- **Sá»­ dá»¥ng**:
  ```python
  from src.config import get_config
  config = get_config()
  embedding_model = config.get('indexing.embedding.model_name')
  ```

### 2. **Ingestion Module** (`src/ingestion/`)
- **Vai trÃ²**: Load corpus vÃ  Q&A dataset, preprocessing text
- **Classes**:
  - `CorpusLoader`: Load vÃ  clean corpus
  - `QADataLoader`: Load train/test Q&A data
- **Flow**: Raw data â†’ Cleaned text

### 3. **Indexing Module** (`src/indexing/`)
- **Vai trÃ²**: Chunk text, táº¡o embeddings, build search indexes
- **Classes**:
  - `TextChunker`: Chia corpus thÃ nh chunks (fixed/semantic/sentence/paragraph)
  - `EmbeddingGenerator`: Táº¡o embeddings vá»›i sentence-transformers
  - `IndexBuilder`: Build FAISS (dense), BM25 (sparse), hoáº·c hybrid index
- **Flow**: Text â†’ Chunks â†’ Embeddings â†’ Index

### 4. **Retrieval Module** (`src/retrieval/`)
- **Vai trÃ²**: Truy xuáº¥t top-K documents liÃªn quan cho query
- **Class**: `Retriever`
  - Dense retrieval: FAISS similarity search
  - Sparse retrieval: BM25 keyword matching
  - Hybrid: Káº¿t há»£p dense + sparse vá»›i weighted fusion
  - Re-ranking: Cross-encoder Ä‘á»ƒ cáº£i thiá»‡n ranking
- **Flow**: Query â†’ Embeddings â†’ Search â†’ Re-rank â†’ Top-K chunks

### 5. **Generation Module** (`src/generation/`)
- **Vai trÃ²**: Sinh cÃ¢u tráº£ lá»i tá»« LLM
- **Classes**:
  - `LLMGenerator`: Wrapper cho HuggingFace models (T5, Llama, GPT, v.v.)
  - `RAGPipeline`: Káº¿t há»£p retrieval + generation
- **Flow**: Query â†’ Retrieve contexts â†’ Build prompt â†’ LLM inference â†’ Answer

### 6. **Evaluation Module** (`src/evaluation/`)
- **Vai trÃ²**: ÄÃ¡nh giÃ¡ predictions vÃ  táº¡o submission file
- **Classes**:
  - `Evaluator`: TÃ­nh BLEU, ROUGE, BERTScore, Exact Match
  - `SubmissionGenerator`: Táº¡o CSV submission cho Kaggle
- **Flow**: Predictions + References â†’ Metrics / Submission CSV

### 7. **Utils Module** (`src/utils/`)
- **Vai trÃ²**: Helper functions
- **Functions**:
  - `setup_logging()`: Cáº¥u hÃ¬nh logging
  - `set_seed()`: Set random seed cho reproducibility
  - `get_device()`: Auto-detect CUDA/MPS/CPU
  - `save_results()` / `load_results()`: Save/load JSON results

---

## ğŸ› ï¸ TÃ¹y chá»‰nh & má»Ÿ rá»™ng

### 1. Sá»­ dá»¥ng fine-tuned embedding model tá»« HuggingFace

Chá»‰nh sá»­a `config.yaml`:
```yaml
indexing:
  embedding:
    model_name: "your-username/your-finetuned-embedding-model"
    backend: "tensorflow"
```

### 2. Sá»­ dá»¥ng fine-tuned LLM

Chá»‰nh sá»­a `config.yaml`:
```yaml
generation:
  model:
    model_name: "your-username/your-finetuned-llm"
    backend: "tensorflow"
    use_mixed_precision: true  # Náº¿u GPU nhá»
```

### 3. Thay Ä‘á»•i chunking strategy

```yaml
indexing:
  chunking:
    strategy: "sentence"  # Thá»­ sentence-based chunking
```

### 4. Äiá»u chá»‰nh retrieval strategy

```yaml
retrieval:
  strategy: "dense"  # Chá»‰ dÃ¹ng dense retrieval
  top_k: 10          # TÄƒng sá»‘ chunks retrieve
```

### 5. Custom prompt template

Chá»‰nh sá»­a trong `config.yaml`:
```yaml
generation:
  prompt:
    template: |
      You are a helpful assistant. Answer concisely.
      
      Context:
      {context}
      
      Question: {question}
      
      Answer:
```

### 6. ThÃªm metrics má»›i

Chá»‰nh sá»­a `src/evaluation/evaluator.py` vÃ  implement metric tÃ¹y chá»‰nh trong class `Evaluator`.

---

## ğŸ§ª Testing

Cháº¡y unit tests:
```bash
pytest tests/
```

---

## ğŸ“Š Experiment Tracking (Optional)

### Sá»­ dá»¥ng Weights & Biases

1. Cáº¥u hÃ¬nh trong `config.yaml`:
```yaml
logging:
  use_wandb: true
  wandb_project: "casml-rag-hackathon"
```

2. Set API key trong `.env`:
```
WANDB_API_KEY=your_wandb_key
```

3. Log metrics tá»± Ä‘á»™ng khi cháº¡y scripts.

---

## ğŸ› ï¸ Troubleshooting

### GPU Out of Memory
- Giáº£m `batch_size` trong config
- Báº­t mixed precision: `use_mixed_precision: true` trong generation config
- Äáº·t `gpu_memory_limit` trong resources config (MB)
- Giáº£m `chunk_size` vÃ  `top_k`

### TensorFlow GPU khÃ´ng nháº­n
- Kiá»ƒm tra CUDA vÃ  cuDNN Ä‘Ã£ cÃ i Ä‘Ãºng phiÃªn báº£n
- Cháº¡y: `python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"`
- Báº­t `gpu_memory_growth: true` trong config

### FAISS Import Error
- CÃ i `faiss-gpu` náº¿u cÃ³ CUDA:
  ```bash
  pip uninstall faiss-cpu
  pip install faiss-gpu
  ```

### Kaggle API Error
- Äáº£m báº£o Ä‘Ã£ accept competition rules trÃªn Kaggle
- Kiá»ƒm tra `KAGGLE_USERNAME` vÃ  `KAGGLE_KEY` trong `.env`

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

- **TensorFlow**: https://www.tensorflow.org/guide
- **HuggingFace Transformers**: https://huggingface.co/docs/transformers
- **Sentence Transformers**: https://www.sbert.net/
- **FAISS**: https://github.com/facebookresearch/faiss
- **LangChain**: https://python.langchain.com/
- **RAG Papers**: [RAG (Lewis et al., 2020)](https://arxiv.org/abs/2005.11401)

---

## ğŸ“„ License

MIT License

---

## ğŸ‘¥ Contributors

CASML Team - INSA Lyon

---

## ğŸ™ Acknowledgments

Cuá»™c thi Ä‘Æ°á»£c tá»• chá»©c bá»Ÿi CASML trÃªn ná»n táº£ng Kaggle.

