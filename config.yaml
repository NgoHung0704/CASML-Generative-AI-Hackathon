# ===============================================
# RAG SYSTEM CONFIGURATION
# CASML Generative AI Hackathon
# ===============================================

# Project Metadata
project:
  name: "CASML-RAG-QA-System"
  version: "1.0.0"
  seed: 42  # For reproducibility

# Data Paths
paths:
  data_dir: "data"
  raw_data: "data/raw"
  processed_data: "data/processed"
  embeddings_dir: "data/embeddings"
  indexes_dir: "data/indexes"
  submissions_dir: "data/submissions"
  
  # Specific files (customize based on competition data)
  corpus_file: "data/raw/corpus.txt"
  train_questions: "data/raw/train_qa.csv"
  test_questions: "data/raw/test_qa.csv"

# Data Ingestion
ingestion:
  # Text preprocessing
  lowercase: true
  remove_special_chars: false
  remove_extra_whitespace: true
  min_text_length: 10
  
  # Document parsing
  encoding: "utf-8"
  chunk_overlap_ratio: 0.1

# Indexing Configuration
indexing:
  # Chunking strategy
  chunking:
    strategy: "recursive"  # Options: "fixed", "semantic", "sentence", "paragraph", "recursive"
    chunk_size: 800  # characters/tokens (from notebook)
    chunk_overlap: 100  # characters/tokens
    separator: "\n\n"
    # Recursive-specific settings
    separators: ["\n\n", "\n", ". ", " ", ""]  # LangChain RecursiveCharacterTextSplitter
  
  # Embedding model
  embedding:
    model_name: "BAAI/bge-large-en-v1.5"  # BGE embedding model (from notebook)
    # For fine-tuned model from HuggingFace:
    # model_name: "your-username/your-finetuned-model"
    backend: "tensorflow"  # TensorFlow backend for sentence-transformers
    device: "GPU:0"  # Options: "GPU:0", "CPU" (TensorFlow format)
    batch_size: 32
    max_seq_length: 512
    normalize_embeddings: true
  
  # Index type
  index:
    type: "hybrid"  # Options: "faiss", "bm25", "hybrid"
    faiss_index_type: "IndexFlatIP"  # Options: "IndexFlatIP", "IndexIVFFlat"
    faiss_nlist: 100  # Number of clusters for IVF
    save_index: true

# Retrieval Configuration
retrieval:
  # Retrieval strategy
  strategy: "hybrid"  # Options: "dense", "sparse", "hybrid"
  top_k: 5
  
  # Hybrid retrieval weights
  dense_weight: 0.6
  sparse_weight: 0.4
  
  # Re-ranking
  use_reranker: true
  reranker_model: "BAAI/bge-reranker-v2-m3"  # FlagReranker model
  rerank_top_k: 8
  rerank_multiplier: 10  # Retrieve 10x documents before reranking
  
  # Query transformation (HyDE - Hypothetical Document Embeddings)
  use_query_transformation: true
  transformation_method: "hyde"  # Options: "hyde", "expansion", "none"

# Generation Configuration
generation:
  # LLM settings
  model:
    provider: "huggingface"  # Options: "huggingface", "openai", "anthropic", "local"
    model_name: "google/flan-t5-base"  # or "meta-llama/Llama-2-7b-chat-hf"
    # For fine-tuned models:
    # model_name: "your-username/your-finetuned-llm"
    backend: "tensorflow"  # Use TensorFlow backend
    device: "GPU:0"  # Options: "GPU:0", "CPU"
    use_mixed_precision: false  # Enable for faster inference on GPU
  
  # Generation parameters
  inference:
    max_new_tokens: 256
    temperature: 0.3
    top_p: 0.9
    top_k: 50
    do_sample: true
    num_beams: 1
    repetition_penalty: 1.2
  
  # Prompt template
  prompt:
    template: |
      Answer the question based on the context below. Be concise and accurate.
      
      Context:
      {context}
      
      Question: {question}
      
      Answer:
    
    max_context_length: 2048  # tokens

# Evaluation Configuration
evaluation:
  metrics:
    - "bleu"
    - "rouge"
    - "bertscore"
    - "exact_match"
  
  # Metric-specific settings
  bleu:
    max_order: 4
  
  rouge:
    rouge_types: ["rouge1", "rouge2", "rougeL"]
  
  bertscore:
    model: "microsoft/deberta-xlarge-mnli"
    use_fast_tokenizer: true

# Experiment Tracking
logging:
  level: "INFO"  # Options: "DEBUG", "INFO", "WARNING", "ERROR"
  log_file: "logs/experiment.log"
  use_wandb: false
  wandb_project: "casml-rag-hackathon"
  wandb_entity: null

# Resource Management
resources:
  max_workers: 4  # For parallel processing
  cache_size: 1000  # Number of items to cache
  gpu_memory_growth: true  # Enable TensorFlow GPU memory growth
  gpu_memory_limit: null  # Set limit in MB, or null for no limit
