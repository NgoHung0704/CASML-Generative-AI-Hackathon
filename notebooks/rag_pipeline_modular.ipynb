{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dafdef39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Project root: d:\\Documents\\INSA_Lyon\\INSA 4A\\TCD 1\\CASML-Generative-AI-Hackathon\n",
      "✓ Added to path: d:\\Documents\\INSA_Lyon\\INSA 4A\\TCD 1\\CASML-Generative-AI-Hackathon\\src\n",
      "✓ Imports configured\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path - go up one directory from notebooks to project root\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "src_path = str(project_root / 'src')\n",
    "\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"✓ Project root: {project_root}\")\n",
    "print(f\"✓ Added to path: {src_path}\")\n",
    "print(\"✓ Imports configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e6c8d",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "Import all necessary libraries for the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af38ea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\torchgpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PDF Processing\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embeddings (using sentence-transformers, no TensorFlow needed)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from FlagEmbedding import FlagReranker\n",
    "\n",
    "# Vector Store\n",
    "import faiss\n",
    "\n",
    "# LLM Model\n",
    "import transformers\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c782d6",
   "metadata": {},
   "source": [
    "## Step 2: Load PDF and Extract TOC\n",
    "\n",
    "1. Load tài liệu (PyPDFLoader tự động thêm 'page' vào metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b0ebc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Prince 14.2 (www.princexml.com)',\n",
       " 'creator': 'PyPDF',\n",
       " 'creationdate': '2022-02-24T09:25:50-06:00',\n",
       " 'moddate': '2022-03-01T11:18:04-06:00',\n",
       " 'title': 'Psychology 2e',\n",
       " 'source': '../data/raw/book.pdf',\n",
       " 'total_pages': 753,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"../data/raw/book.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Kiểm tra thử 1 trang xem có metadata chưa\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2408d0cf",
   "metadata": {},
   "source": [
    "2. Chia nhỏ văn bản (Chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ec067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk size không nên quá nhỏ để giữ ngữ cảnh, overlap giúp không bị cắt giữa câu\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Lưu ý: Khi split, LangChain tự động sao chép metadata (số trang) từ document gốc sang từng chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b30561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2963"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea83851",
   "metadata": {},
   "source": [
    "## Step 3: Load Embedding Model\n",
    "\n",
    "Using BGE-large-en-v1.5 (sentence-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934610e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded embedding model: BAAI/bge-large-en-v1.5\n",
      "✓ Embedding dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "# Load BGE embedding model\n",
    "model_emb_name = \"BAAI/bge-large-en-v1.5\"\n",
    "embedding_model = SentenceTransformer(model_emb_name)\n",
    "\n",
    "print(f\"✓ Loaded embedding model: {model_emb_name}\")\n",
    "print(f\"✓ Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bf3a2",
   "metadata": {},
   "source": [
    "## Step 4: Generate Embeddings for Chunks\n",
    "\n",
    "Encode all text chunks using BGE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c1ab73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 2963 chunks...\n",
      "This may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 93/93 [04:54<00:00,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated embeddings shape: (2963, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract text from chunks\n",
    "chunk_texts = [doc.page_content for doc in all_splits]\n",
    "\n",
    "print(f\"Generating embeddings for {len(chunk_texts)} chunks...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Generate embeddings with BGE\n",
    "# BGE recommends adding instruction for queries, but not for passages\n",
    "embeddings = embedding_model.encode(\n",
    "    chunk_texts,\n",
    "    normalize_embeddings=True,  # Normalize for cosine similarity\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"✓ Generated embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9819fcc",
   "metadata": {},
   "source": [
    "## Step 5: Build FAISS Index\n",
    "\n",
    "Create vector index for fast similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6688dea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ FAISS index created\n",
      "  Dimension: 1024\n",
      "  Total vectors: 2963\n"
     ]
    }
   ],
   "source": [
    "# Get embedding dimension\n",
    "embedding_dim = embeddings.shape[1]\n",
    "\n",
    "# Create FAISS index (Inner Product for normalized vectors = Cosine Similarity)\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "\n",
    "# Add embeddings to index\n",
    "index.add(embeddings.astype('float32')) # Reduced precision for FAISS\n",
    "\n",
    "print(f\"✓ FAISS index created\")\n",
    "print(f\"  Dimension: {embedding_dim}\")\n",
    "print(f\"  Total vectors: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f027d4",
   "metadata": {},
   "source": [
    "## Step 6: Test Retrieval\n",
    "\n",
    "Search for relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a8ab78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the contributions made by Freud in psychology?\n",
      "\n",
      "Top 5 results:\n",
      "======================================================================\n",
      "\n",
      "[1] Score: 0.6960\n",
      "Page: 373\n",
      "Content: • Define and describe the nature and function of the id, ego, and superego\n",
      "• Define and describe the defense mechanisms\n",
      "• Define and describe the psychosexual stages of personality development\n",
      "Sigmund...\n",
      "\n",
      "[2] Score: 0.6930\n",
      "Page: 22\n",
      "Content: the unconscious mind could be accessed through dream analysis, by examinations of the first words that came\n",
      "to people’s minds, and through seemingly innocent slips of the tongue. Psychoanalytic theory...\n",
      "\n",
      "[3] Score: 0.6880\n",
      "Page: 22\n",
      "Content: FIGURE 1.3 William James, shown here in a self-portrait, was the first American psychologist.\n",
      "Freud and Psychoanalytic Theory\n",
      "Perhaps one of the most influential and well-known figures in psychology’s...\n",
      "\n",
      "[4] Score: 0.6827\n",
      "Page: 692\n",
      "Content: Freud, S. (1920). Resistance and suppression. A general introduction to psychoanalysis (pp. 248–261). Horace\n",
      "Liveright.\n",
      "Freud, S. (1923/1949). The ego and the id. Hogarth.\n",
      "Freud, S. (1931/1968). Femal...\n",
      "\n",
      "[5] Score: 0.6760\n",
      "Page: 52\n",
      "Content: feeling of fear. A hypothesis that could be derived from this theory might be that a person who is unaware of\n",
      "the physiological arousal that the sight of the snake elicits will not feel fear.\n",
      "A scient...\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "test_query = \"What are the contributions made by Freud in psychology?\"\n",
    "\n",
    "# For BGE, add instruction prefix for queries\n",
    "query_instruction = \"Represent this sentence for searching relevant passages: \"\n",
    "query_with_instruction = query_instruction + test_query\n",
    "\n",
    "# Encode query\n",
    "query_embedding = embedding_model.encode(\n",
    "    [query_with_instruction],\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "# Search in FAISS\n",
    "k = 5  # Top-5 results\n",
    "distances, indices = index.search(query_embedding.astype('float32'), k)\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(f\"Top {k} results:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (idx, score) in enumerate(zip(indices[0], distances[0])):\n",
    "    print(f\"\\n[{i+1}] Score: {score:.4f}\")\n",
    "    print(f\"Page: {all_splits[idx].metadata.get('page', 'N/A')}\")\n",
    "    print(f\"Content: {chunk_texts[idx][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3454dea",
   "metadata": {},
   "source": [
    "## Step 7: Load Reranker Model\n",
    "\n",
    "Use FlagReranker to improve retrieval quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d00efd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded reranker: BAAI/bge-reranker-v2-m3\n"
     ]
    }
   ],
   "source": [
    "# Load reranker\n",
    "reranker_model = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)\n",
    "\n",
    "print(f\"✓ Loaded reranker: BAAI/bge-reranker-v2-m3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c0658",
   "metadata": {},
   "source": [
    "## Step 8: Retrieval with Reranking\n",
    "\n",
    "Retrieve more candidates, then rerank for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff84a051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the contributions made by Freud in psychology?\n",
      "\n",
      "Top 5 results (after reranking):\n",
      "============================================================\n",
      "\n",
      "[1] Rerank Score: 0.8440\n",
      "Page: 295\n",
      "Content: nurturance and parenting during a stage, we may become stuck, or fixated, in that stage. Freud’s stages are\n",
      "called the stages of psychosexual development. According to Freud, children’s pleasure-seeki...\n",
      "\n",
      "[2] Rerank Score: 0.7160\n",
      "Page: 23\n",
      "Content: interactions, and the development of personality over time. Westen identifies subsequent research support for\n",
      "all of these ideas.\n",
      "More modern iterations of Freud’s clinical approach have been empirica...\n",
      "\n",
      "[3] Rerank Score: 0.5871\n",
      "Page: 33\n",
      "Content: people in this country will be 65 or older (Department of Health and Human Services, n.d.).\n",
      "Personality Psychology\n",
      "Personality psychology focuses on patterns of thoughts and behaviors that make each i...\n",
      "\n",
      "[4] Rerank Score: 0.5460\n",
      "Page: 380\n",
      "Content: negative dynamics of the youngest and oldest children. Despite popular attention, research has not\n",
      "conclusively confirmed Adler’s hypotheses about birth order.\n",
      "LINK T O LEARNING\n",
      "One of Adler’s major c...\n",
      "\n",
      "[5] Rerank Score: 0.4887\n",
      "Page: 22\n",
      "Content: the unconscious mind could be accessed through dream analysis, by examinations of the first words that came\n",
      "to people’s minds, and through seemingly innocent slips of the tongue. Psychoanalytic theory...\n"
     ]
    }
   ],
   "source": [
    "# Retrieve more candidates for reranking\n",
    "k_faiss = 50  # Retrieve 50 candidates\n",
    "k_reRanker = 5     # Return top 5 after reranking\n",
    "\n",
    "# Search\n",
    "distances, indices = index.search(query_embedding.astype('float32'), k_faiss)\n",
    "\n",
    "# Prepare pairs for reranking: [[query, passage1], [query, passage2], ...]\n",
    "pairs = [[test_query, chunk_texts[idx]] for idx in indices[0]]\n",
    "\n",
    "# Rerank\n",
    "rerank_scores = reranker_model.compute_score(pairs, normalize=True)\n",
    "\n",
    "# Sort by rerank scores\n",
    "ranked_results = sorted(\n",
    "    zip(indices[0], rerank_scores),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:k_reRanker]\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(f\"Top {k_reRanker} results (after reranking):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, (idx, score) in enumerate(ranked_results):\n",
    "    print(f\"\\n[{i+1}] Rerank Score: {score:.4f}\")\n",
    "    print(f\"Page: {all_splits[idx].metadata.get('page', 'N/A')}\")\n",
    "    print(f\"Content: {chunk_texts[idx][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5defdd0b",
   "metadata": {},
   "source": [
    "# Step 9 : LLM Model (LLama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "439d4655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM model: Qwen/Qwen2.5-1.5B-Instruct...\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\torchgpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ADMIN\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-1.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LLM model loaded successfully on cuda\n",
      "✓ Model size: ~1.5B parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load LLM model and tokenizer\n",
    "# Using Qwen2.5 (open-source, no gating, good performance)\n",
    "model_llm_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "print(f\"Loading LLM model: {model_llm_name}...\")\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_llm_name)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_llm_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"✓ LLM model loaded successfully on {device}\")\n",
    "print(f\"✓ Model size: ~1.5B parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20963ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the contributions made by Freud in psychology?\n",
      "\n",
      "Generated Answer:\n",
      "Sigmund Freud contributed significantly to psychology's understanding of the human psyche through his theories on the id, ego, and superego, defense mechanisms, and psychosexual stages of personality development. He proposed that the unconscious mind plays a critical role in shaping behavior and emotions, advocating for techniques like dream analysis and treatment methods based on his psychoanalytic theory. His work influenced clinical psychology and remains relevant today.\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(query: str, contexts: list, max_length: int = 512) -> str:\n",
    "    \"\"\"\n",
    "    Generate answer using LLM based on query and retrieved contexts.\n",
    "    \n",
    "    Args:\n",
    "        query: User question\n",
    "        contexts: List of relevant context strings\n",
    "        max_length: Maximum tokens for generation\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer string\n",
    "    \"\"\"\n",
    "    # Format prompt with contexts\n",
    "    context_text = \"\\n\\n\".join([f\"Context {i+1}:\\n{ctx}\" for i, ctx in enumerate(contexts)])\n",
    "    \n",
    "    prompt = f\"\"\"Using the following contexts, answer the question concisely and accurately.\n",
    "\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and extract answer\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = full_response.split(\"Answer:\")[-1].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test generation\n",
    "test_contexts = [chunk_texts[idx] for idx in indices[0][:3]]\n",
    "test_answer = generate_answer(test_query, test_contexts)\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"\\nGenerated Answer:\\n{test_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858c7d90",
   "metadata": {},
   "source": [
    "## Step 10: Process Multiple Queries\n",
    "\n",
    "Load queries from JSON and generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcd61fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading queries from ../data/raw/queries.json...\n",
      "✓ Loaded 50 queries\n",
      "Processing queries and generating answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 50/50 [1:56:58<00:00, 140.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Processing complete!\n",
      "✓ Saved results to: ../data/outputs/submission.csv\n",
      "✓ Total queries processed: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load queries\n",
    "queries_file = \"../data/raw/queries.json\"\n",
    "output_file = \"../data/outputs/submission.csv\"\n",
    "\n",
    "print(f\"Loading queries from {queries_file}...\")\n",
    "with open(queries_file, 'r', encoding='utf-8') as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(queries)} queries\")\n",
    "print(f\"Processing queries and generating answers...\")\n",
    "\n",
    "# Create/overwrite CSV with header\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"ID\", \"context\", \"answer\", \"references\"])\n",
    "\n",
    "# Process each query\n",
    "for query_data in tqdm(queries, desc=\"Processing queries\"):\n",
    "    query_id = query_data[\"query_id\"]\n",
    "    question = query_data[\"question\"]\n",
    "    \n",
    "    # Step 1: Encode query with BGE instruction\n",
    "    query_instruction = \"Represent this sentence for searching relevant passages: \"\n",
    "    query_with_instruction = query_instruction + question\n",
    "    query_embedding = embedding_model.encode(\n",
    "        [query_with_instruction],\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    \n",
    "    # Step 2: Retrieve candidates from FAISS\n",
    "    k_candidates = 50\n",
    "    distances, indices = index.search(query_embedding.astype('float32'), k_candidates)\n",
    "    \n",
    "    # Step 3: Rerank with FlagReranker\n",
    "    pairs = [[question, chunk_texts[idx]] for idx in indices[0]]\n",
    "    rerank_scores = reranker_model.compute_score(pairs, normalize=True)\n",
    "    \n",
    "    # Get top 5 after reranking\n",
    "    k_final = 5\n",
    "    ranked_results = sorted(\n",
    "        zip(indices[0], rerank_scores),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:k_final]\n",
    "    \n",
    "    # Step 4: Get contexts and generate answer\n",
    "    top_contexts = [chunk_texts[idx] for idx, score in ranked_results]\n",
    "    answer = generate_answer(question, top_contexts, max_length=256)\n",
    "    \n",
    "    # Step 5: Get references (page numbers)\n",
    "    references = [all_splits[idx].metadata.get('page', 'N/A') for idx, score in ranked_results]\n",
    "    references_str = ','.join(map(str, references))\n",
    "    \n",
    "    # Step 6: Combine contexts\n",
    "    context_combined = \"\\n\\n\".join(top_contexts)\n",
    "    \n",
    "    # Step 7: Save to CSV\n",
    "    with open(output_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([query_id, context_combined, answer, references_str])\n",
    "\n",
    "print(f\"\\n✓ Processing complete!\")\n",
    "print(f\"✓ Saved results to: {output_file}\")\n",
    "print(f\"✓ Total queries processed: {len(queries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e8d2cf",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Pipeline completed with:\n",
    "1. ✅ PDF loading with LangChain PyPDFLoader\n",
    "2. ✅ Recursive text chunking (1000 chars, 200 overlap)\n",
    "3. ✅ BGE embeddings (BAAI/bge-large-en-v1.5)\n",
    "4. ✅ FAISS vector indexing\n",
    "5. ✅ Semantic search with cosine similarity\n",
    "6. ✅ FlagReranker (BAAI/bge-reranker-v2-m3)\n",
    "7. ✅ Two-stage retrieval: FAISS → Reranking\n",
    "\n",
    "**Next Steps:**\n",
    "- Add LLM for answer generation\n",
    "- Implement TOC extraction for references\n",
    "- Create batch processing for multiple queries\n",
    "- Save results to CSV for Kaggle submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
